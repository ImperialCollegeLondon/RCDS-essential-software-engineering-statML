{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding JAX's JIT Compilation\n",
    "\n",
    "This notebook explains how Just-In-Time (JIT) compilation works in JAX, with examples and explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is JAX?\n",
    "\n",
    "JAX is a high-performance numerical computing library developed by Google. It combines NumPy's familiar API with the benefits of automatic differentiation and GPU/TPU acceleration. The name \"JAX\" comes from \"Just After eXecution\" - reflecting its ability to transform and optimize code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install JAX if not already installed\n",
    "# !pip install jax jaxlib\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT Compilation in JAX\n",
    "\n",
    "JIT stands for \"Just-In-Time\" compilation. In JAX, JIT compilation transforms your Python code into optimized, compiled code that can run much faster and more efficiently on accelerators like GPUs and TPUs.\n",
    "\n",
    "### How JIT Works in JAX\n",
    "\n",
    "When you apply the `@jax.jit` decorator to a function, JAX performs the following steps:\n",
    "\n",
    "1. **Tracing**: JAX traces the execution of your function using abstract values to determine its structure.\n",
    "2. **XLA Conversion**: JAX converts the traced function into XLA (Accelerated Linear Algebra) operations.\n",
    "3. **Compilation**: The XLA operations are compiled into optimized machine code for your target device (CPU, GPU, or TPU).\n",
    "4. **Caching**: The compiled result is cached, so subsequent calls with inputs of the same shape and type use the cached compiled version.\n",
    "\n",
    "This process happens \"just in time\" - when the function is first called, not when it's defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage of JIT\n",
    "\n",
    "The simplest way to use JIT in JAX is with the `@jax.jit` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular function (not JIT-compiled)\n",
    "def slow_function(x, y):\n",
    "    return jnp.dot(x, y)\n",
    "\n",
    "# JIT-compiled function\n",
    "@jax.jit\n",
    "def fast_function(x, y):\n",
    "    return jnp.dot(x, y)\n",
    "\n",
    "# Let's compare performance\n",
    "x = jnp.ones((1000, 1000))\n",
    "y = jnp.ones((1000, 1000))\n",
    "\n",
    "# Warm-up\n",
    "_ = slow_function(x, y)\n",
    "_ = fast_function(x, y)  # First call includes compilation time\n",
    "\n",
    "# Timing comparison\n",
    "start = time.time()\n",
    "_ = slow_function(x, y)\n",
    "regular_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = fast_function(x, y)\n",
    "jit_time = time.time() - start\n",
    "\n",
    "print(f\"Regular function time: {regular_time:.6f} seconds\")\n",
    "print(f\"JIT function time: {jit_time:.6f} seconds\")\n",
    "print(f\"Speedup: {regular_time/jit_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How JIT Transforms Your Code\n",
    "\n",
    "JIT works by creating a computation graph of your function operations. This process has several key aspects:\n",
    "\n",
    "### 1. Function Purity Requirements\n",
    "\n",
    "JIT requires **pure functions** that:\n",
    "- Have no side effects (don't modify external state)\n",
    "- Return the same output for the same input\n",
    "- Don't use Python control flow based on values that aren't known during tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will work with JIT\n",
    "@jax.jit\n",
    "def good_function(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "# This will NOT work properly with JIT\n",
    "counter = 0\n",
    "@jax.jit\n",
    "def bad_function(x):  \n",
    "    global counter\n",
    "    counter += 1  # Side effect!\n",
    "    if x[0] > 0:  # Control flow based on values not known during tracing\n",
    "        return jnp.sum(x)\n",
    "    else:\n",
    "        return jnp.mean(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Static vs Dynamic Variables\n",
    "\n",
    "JAX distinguishes between two types of arguments:\n",
    "\n",
    "- **Traced/dynamic arguments**: Regular JAX arrays that are part of the computation\n",
    "- **Static arguments**: Values that are fixed during tracing and affect the structure of the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using static_argnums to specify static arguments\n",
    "@jax.jit(static_argnums=(1,))\n",
    "def create_matrix(x, size):\n",
    "    return jnp.ones((size, size)) * x\n",
    "\n",
    "# Now size is treated as static - a new function is compiled for each different size\n",
    "small = create_matrix(2.0, 3)  # Compiles for size=3\n",
    "large = create_matrix(2.0, 5)  # Compiles again for size=5\n",
    "\n",
    "print(f\"Small matrix:\\n{small}\")\n",
    "print(f\"\\nLarge matrix:\\n{large}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Handling Python Control Flow\n",
    "\n",
    "Regular Python control flow (if/else, loops) that depends on dynamic values doesn't work well with JIT because the function is traced once with abstract values. JAX provides special control flow primitives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using JAX's control flow operations\n",
    "@jax.jit\n",
    "def absolute_value(x):\n",
    "    return jax.lax.cond(\n",
    "        x >= 0,          # Predicate\n",
    "        lambda _: x,     # True branch\n",
    "        lambda _: -x,    # False branch\n",
    "        operand=None     # Additional argument passed to branches (not used here)\n",
    "    )\n",
    "\n",
    "print(f\"abs(5) = {absolute_value(jnp.array(5))}\")\n",
    "print(f\"abs(-3) = {absolute_value(jnp.array(-3))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Using JIT\n",
    "\n",
    "1. **Performance**: JIT compilation can make your code run much faster, especially for numerical operations.\n",
    "2. **GPU/TPU Acceleration**: JIT enables efficient execution on accelerator hardware.\n",
    "3. **Optimization**: XLA performs many optimizations that wouldn't be possible in Python.\n",
    "4. **Fusion**: Operations are fused together, reducing memory transfers.\n",
    "\n",
    "## Limitations and Gotchas\n",
    "\n",
    "1. **Compilation Time**: The first call to a JIT function includes compilation time, which can be significant.\n",
    "2. **Python Features**: Not all Python constructs are supported within JIT-compiled functions.\n",
    "3. **Debugging**: Debugging JIT-compiled code can be more difficult.\n",
    "4. **Recompilation**: Changes in input shapes can trigger recompilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration of recompilation due to shape changes\n",
    "@jax.jit\n",
    "def matrix_multiply(a, b):\n",
    "    return jnp.dot(a, b)\n",
    "\n",
    "# First call with shape (2, 3) and (3, 2)\n",
    "a1 = jnp.ones((2, 3))\n",
    "b1 = jnp.ones((3, 2))\n",
    "print(\"First call (compiles)...\")\n",
    "start = time.time()\n",
    "result1 = matrix_multiply(a1, b1)\n",
    "print(f\"Time: {time.time() - start:.6f} seconds\")\n",
    "\n",
    "# Second call with same shapes (uses cached compilation)\n",
    "print(\"\\nSecond call with same shapes (uses cached version)...\")\n",
    "start = time.time()\n",
    "result2 = matrix_multiply(a1, b1)\n",
    "print(f\"Time: {time.time() - start:.6f} seconds\")\n",
    "\n",
    "# Third call with different shapes (triggers recompilation)\n",
    "a2 = jnp.ones((4, 5))\n",
    "b2 = jnp.ones((5, 4))\n",
    "print(\"\\nThird call with different shapes (recompiles)...\")\n",
    "start = time.time()\n",
    "result3 = matrix_multiply(a2, b2)\n",
    "print(f\"Time: {time.time() - start:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced JIT Features\n",
    "\n",
    "### 1. Explicit Annotations\n",
    "\n",
    "You can control which arguments are static (fixed during compilation):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using static_argnums and static_argnames\n",
    "@jax.jit(static_argnums=(1,), static_argnames=['use_bias'])\n",